---
title: Running Teleport with a Custom Configuration using Helm
description: Install and configure a Teleport cluster with a custom configuration using Helm
---

In this guide, we'll explain how to set up a Teleport cluster in Kubernetes
with a custom [`teleport.yaml`](../../reference/config.mdx) config file
using Teleport Helm charts.

This setup can be useful when you already have an existing Teleport cluster and would
like to start running it in Kubernetes, or when migrating your setup from a legacy
version of the Helm charts.

<Admonition type="warning">
Those instructions are both for v12 Teleport and the v12 `teleport-cluster` chart.
If you are running an older Teleport version, use the version selector at the top
of this page to choose the correct version.
</Admonition>

## Prerequisites

(!docs/pages/kubernetes-access/helm/includes/teleport-cluster-prereqs.mdx!)

## Step 1/4. Install Helm

(!docs/pages/kubernetes-access/helm/includes/teleport-cluster-install.mdx!)

## Step 2/4. Add the Teleport Helm chart repository

(!docs/pages/kubernetes-access/helm/includes/helm-repo-add.mdx!)

## Step 3/4. Setting up a Teleport cluster with Helm using a custom config

In `scratch` mode, the `teleport-cluster` Helm chart generates a minimal
configuration and lets you pass your custom configuration through the chart's values.

`teleport-cluster` deploys two sets of pods: proxy and auth.
You must provide two configurations, one for each pod type.

- The `proxy` pod configuration should contain at least the `proxy_service`
  section and the `teleport` section without the `storage` part.
- The `auth` pod configuration should contain at least the `auth_service` and
  `teleport` sections.

The chart automatically creates a Kubernetes join token, named after the Helm
release, which will enable the proxy pods to seamlessly connect to the auth pods.
If you do not want to use this automatic token, you must provide a valid
Teleport join token in the proxy pods' configuration.

<Admonition type="warning">
When using `scratch` or `standalone` mode, you **must** use highly-available
storage (e.g. etcd, DynamoDB, or Firestore) for multiple replicas to be supported.

[Information on supported Teleport storage backends](../../reference/backends.mdx)

Manually configuring NFS-based storage or `ReadWriteMany` volume claims is **NOT**
supported for an HA deployment and will result in errors.
</Admonition>

Write the following `my-values.yaml` file, and adapt the teleport configuration as needed.
You can find all possible configuration fields in the [Teleport Config Reference](../../reference/config.mdx).

```yaml
chartMode: scratch

auth:
  teleportConfig:
    # put your teleport.yaml auth configuration here
    teleport:
      log:
        output: stderr
        severity: INFO

    auth_service:
      enabled: true
      listen_addr: 0.0.0.0:3025

proxy:
  teleportConfig:
    # put your teleport.yaml proxy configuration here
    teleport:
      # The join_params section must be provided for the proxies to join the auth servers
      # By default, the chart creates a Kubernetes join token which you can use.
      join_params:
        method: kubernetes
        # The token name pattern is "<RELEASE-NAME>-proxy"
        # Change this if you change the Helm release name.
        token_name: "teleport-proxy"
      # The auth server domain pattern is "<RELEASE-NAME>-auth.<RELEASE-NAMESPACE>.svc.cluster.local:3025"
      # If you change the Helm release name or namespace you must adapt the `auth_server` value.
      auth_server: "teleport-auth.teleport.svc.cluster.local:3025"
      log:
        output: stderr
        severity: INFO
    proxy_service:
      enabled: true
      listen_addr: 0.0.0.0:3080
      public_addr: custom.example.com:443

# OPTIONAL - when using an highly-available storage for both backend AND session recordings
# you can disable disk persistence and replicate auth pods.
#
# persistence:
#   enabled: false
# highAvailability:
#   replicaCount: 2
```

You can control the externally-facing name of your cluster using the `public_addr`
sections of `teleport.yaml`. In this example, our `public_addr`s are set to
`custom.example.com`.

<ScopedBlock scope="enterprise">

Before you can install Teleport in your Kubernetes cluster, you will need to
create a secret that contains your Teleport license information.

Download your Teleport Enterprise license from the
[Customer Portal](https://dashboard.gravitational.com/web/login) and save it to
a file called `license.pem`.

Create a secret from your license file. Teleport will automatically discover
this secret as long as your file is named `license.pem`.

```code
$ kubectl -n teleport create secret generic license --from-file=license.pem
```

</ScopedBlock>

<Admonition type="note" title="External proxy port">
Note that although the `proxy_service` listens on port 3080 inside the pod,
the default `LoadBalancer` service configured by the chart will always listen
externally on port 443 (which is redirected internally to port 3080).

Due to this, your `proxy_service.public_addr` should always end in `:443`:

```yaml
proxy_service:
  listen_addr: 0.0.0.0:3080
  public_addr: custom.example.com:443
```

</Admonition>

You can now deploy Teleport in your cluster with the command:

<Tabs>
<TabItem scope={["oss"]} label="Open Source">

```code
$ helm install teleport teleport/teleport-cluster \
  --namespace teleport \
  --values my-values.yaml
```

</TabItem>

<TabItem scope={["enterprise"]} label="Enterprise">

```code
$ helm install teleport teleport/teleport-cluster \
  --namespace teleport \
  --set enterprise=true \
  --values my-values.yaml
```

</TabItem>
</Tabs>

Once the chart is installed, you can use `kubectl` commands to view the deployment:

```code
$ kubectl --namespace teleport get all

NAME                                 READY   STATUS    RESTARTS   AGE
pod/teleport-auth-57989d4cbd-rtrzn   1/1     Running   0          22h
pod/teleport-proxy-c6bf55cfc-w96d2   1/1     Running   0          22h
pod/teleport-proxy-c6bf55cfc-z256w   1/1     Running   0          22h

NAME                        TYPE           CLUSTER-IP     EXTERNAL-IP     PORT(S)                                                                     AGE
service/teleport            LoadBalancer   10.40.11.180   34.138.177.11   443:30258/TCP,3023:31802/TCP,3026:32182/TCP,3024:30101/TCP,3036:30302/TCP   22h
service/teleport-auth       ClusterIP      10.40.8.251    <none>          3025/TCP,3026/TCP                                                           22h
service/teleport-auth-v11   ClusterIP      None           <none>          <none>                                                                      22h
service/teleport-auth-v12   ClusterIP      None           <none>          <none>                                                                      22h

NAME                             READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/teleport-auth    1/1     1            1           22h
deployment.apps/teleport-proxy   2/2     2            2           22h

NAME                                       DESIRED   CURRENT   READY   AGE
replicaset.apps/teleport-auth-57989d4cbd   1         1         1       22h
replicaset.apps/teleport-proxy-c6bf55cfc   2         2         2       22h
```

## Step 4/4. Create a Teleport user (optional)

If you're not migrating an existing Teleport cluster, you'll need to create a
user to be able to log into Teleport. This needs to be done on the Teleport
auth server, so we can run the command using `kubectl`:

```code
$ kubectl --namespace teleport exec deployment/teleport-auth -- tctl users add test --roles=access,editor

User "test" has been created but requires a password. Share this URL with the user to complete user setup, link is valid for 1h:
https://custom.example.com:443/web/invite/91cfbd08bc89122275006e48b516cc68

NOTE: Make sure custom.example.com:443 points at a Teleport proxy that users can access.
```

<Admonition type="note">
If you didn't set up DNS for your hostname earlier, remember to replace
`custom.example.com` with the external IP or hostname of the Kubernetes load
balancer.

(!docs/pages/kubernetes-access/helm/includes/kubernetes-externaladdress.mdx!)

You should modify your command accordingly and replace `custom.example.com` with
either the IP or hostname depending on which you have available. You may need
to accept insecure warnings in your browser to view the page successfully.
</Admonition>

<Admonition type="warning">
Using a Kubernetes-issued load balancer IP or hostname is OK for testing but is
not viable when running a production Teleport cluster as the Subject Alternative
Name on any public-facing certificate will be expected to match the cluster's
configured public address (specified using `public_addr` in your configuration)

You must configure DNS properly using the methods described above for production workloads.
</Admonition>

Load the user creation link to create a password and set up 2-factor
authentication for the Teleport user via the web UI.

## Uninstalling the Helm chart

To uninstall the `teleport-cluster` chart, use `helm uninstall <release-name>`. For example:

```code
$ helm --namespace teleport uninstall teleport
```

<Admonition type="note">
To change `chartMode`, you must first uninstall the existing chart and then
install a new version with the appropriate values.
</Admonition>

## Next steps

To see all of the options you can set in the values file for the
`teleport-cluster` Helm chart, consult our [reference
guide](../../reference/helm-reference/teleport-cluster.mdx).

You can follow our [Getting Started with Teleport guide](../../management/guides/docker.mdx#step-34-creating-a-teleport-user)
to finish setting up your Teleport cluster.
