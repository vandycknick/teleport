---
title: Migrating from a legacy version of the teleport Helm chart
description: How to migrate a Teleport cluster using the legacy teleport helm chart to the teleport-cluster Helm chart
---

In this guide, we'll detail a way to migrate an existing Teleport cluster using the legacy `teleport` Helm chart
to use the newer `teleport-cluster` Helm chart instead.

<Admonition type="warning">
  This guide details a very simple migration scenario for a smaller Teleport cluster which is not deployed for high availability.

  If your Teleport cluster is required to support many users and should be deployed in a highly available configuration, you should
  consider [following a different guide](../helm-deployments.mdx) and storing your cluster's data in AWS DynamoDB or Google Cloud Firestore.
</Admonition>

<ScopedBlock scope="cloud">

(!docs/pages/kubernetes-access/helm/includes/teleport-cluster-cloud-warning.mdx!)

You can also view this guide as a user of another Teleport edition:

</ScopedBlock>

## Prerequisites

(!docs/pages/kubernetes-access/helm/includes/teleport-cluster-prereqs.mdx!)

## Step 1/6. Install Helm

(!docs/pages/kubernetes-access/helm/includes/teleport-cluster-install.mdx!)

## Step 2/6. Add the Teleport Helm chart repository

(!docs/pages/kubernetes-access/helm/includes/helm-repo-add.mdx!)

## Step 3/6. Get the Teleport configuration file from your existing cluster

<Admonition type="tip" title="Teleport storage in AWS or GCP">
  If your Teleport cluster's database is currently stored in AWS DynamoDB or Google Cloud Firestore rather than
  using a `PersistentVolumeClaim` or similar, you may wish to consider redeploying your cluster using the `aws` or `gcp`
  modes of the `teleport-cluster` chart instead.

  The relevant guides are linked here:

  - AWS: [Running an HA Teleport cluster using an AWS EKS Cluster](./aws.mdx)
  - GCP: [Running an HA Teleport cluster using a Google Cloud GKE cluster](./gcp.mdx)
</Admonition>

<Admonition type="note" title="Note on namespacing">
  This guide assumes that your old Helm release was called `teleport` and it is in the `teleport` Kubernetes namespace. If your release
  is different, you will need to update all `kubectl` commands accordingly.
</Admonition>

The first thing you'll need to do is extract the Teleport config file for your existing Teleport cluster.

Firstly, check that the `ConfigMap` is present:

```code
$ kubectl --namespace teleport get configmap/teleport -o yaml

# apiVersion: v1
# data:
#  teleport.yaml: |
#    teleport:
#      log:
#        severity: INFO
#        output: stderr
#      storage:
#        type: dir
# ...
```

<Admonition type="note">
  If you do not see the `teleport` `ConfigMap`, double-check that your Kubernetes context is set correctly and that
  you are using the correct namespace.
</Admonition>

If you see a Teleport config under the `teleport.yaml` key, you can extract it to disk with a command like this:

```code
$ kubectl --namespace teleport get configmap/teleport -o=jsonpath="{.data['teleport\.yaml']}" > teleport.yaml

cat teleport.yaml
# teleport:
#   log:
#    severity: INFO
#    output: stderr
#  storage:
#    type: dir
# ...
```

Once you have the config, you should upload this to a separate Kubernetes
namespace (where you intend to run the `teleport-cluster` chart).

```code
$ kubectl create namespace teleport-cluster
# namespace/teleport-cluster created
$ kubectl --namespace teleport-cluster create configmap teleport --from-file=teleport.yaml
# configmap/teleport created
```

## Step 4/6. Extracting the contents of Teleport's database

<Admonition type="note" title="Note on namespacing">
  If you migrate your existing data, the `cluster_name` which is configured in `teleport.yaml` must stay the same.

  If you wish to change the name of your cluster, you will need to deploy a new cluster from scratch and reconfigure your
  users, roles and nodes.
</Admonition>

If you wish to keep the same users, roles, certificate authorities and nodes in your cluster, you can use
Teleport's `tctl` tool to extract a backup of all your data.

You can get the backup with a command like this:

```code
$ kubectl --namespace teleport exec deploy/teleport -- tctl get all --with-secrets > backup.yaml
```

<Admonition type="warning">
  The `backup.yaml` file you have just written contains private keys for your Teleport cluster's certificate
  authorities in plain text. You must protect this file carefully and delete it once your new cluster is running.

  You can write the file to an in-memory `tmpfs` like `/dev/shm/backup.yaml` for greater security.
</Admonition>

Add the backup to your new `teleport-cluster` namespace as a secret:

```code
$ kubectl --namespace teleport-cluster create secret generic bootstrap --from-file=backup.yaml
```

## Step 5/6. Start the new cluster with your old config file and backup

We will start the new cluster and bootstrap it using the backup of your
cluster's data. Once this step is complete and the cluster is working, we'll
modify the deployment to remove references to the backup data, and remove it
from Kubernetes for security.

<Tabs>
  <TabItem scope="oss" label="Teleport Community Edition">
  Write a `teleport-cluster-values.yaml` file containing the following values:

  ```yaml
  chartMode: custom
  extraArgs: ['--bootstrap', '/etc/teleport-bootstrap/backup.yaml']
  extraVolumes:
  - name: bootstrap
    secret:
      name: bootstrap
  extraVolumeMounts:
  - name: bootstrap
    path: /etc/teleport-bootstrap
  ```

  ```code
  $ helm install teleport teleport/teleport-cluster \
    --namespace teleport-cluster \
    -f teleport-cluster-values.yaml
  ```

  </TabItem>
  <TabItem scope="enterprise" label="Teleport Enterprise">

  Obtain your Teleport Enterprise license file from the [Teleport Customer
  Portal](https://dashboard.gravitational.com/web/login). Create a secret called
  "license" in the namespace you created:

  ```code
  $ kubectl -n teleport-cluster create secret generic license --from-file=license.pem
  ```

  Write a `teleport-cluster-values.yaml` file containing the following values:

  ```yaml
  enterprise: true
  chartMode: custom
  extraArgs: ['--bootstrap', '/etc/teleport-bootstrap/backup.yaml']
  extraVolumes:
  - name: bootstrap
    secret:
      name: bootstrap
  extraVolumeMounts:
  - name: bootstrap
    path: /etc/teleport-bootstrap
  ```

  ```code
  $ helm install teleport teleport/teleport-cluster \
    --namespace teleport-cluster \
    -f teleport-cluster-values.yaml
  ```

  </TabItem>
</Tabs>

Once the chart is installed, you can use `kubectl` commands to view the deployment:

```code
$ kubectl --namespace teleport-cluster get all

# NAME                            READY   STATUS    RESTARTS   AGE
# pod/teleport-5cf46ddf5f-dzh65   1/1     Running   0          4m21s
# pod/teleport-5cf46ddf5f-mpghq   1/1     Running   0          4m21s

# NAME               TYPE           CLUSTER-IP      EXTERNAL-IP                                                               PORT(S)                                                      AGE
# service/teleport   LoadBalancer   10.100.37.171   a232d92df01f940339adea0e645d88bb-1576732600.us-east-1.elb.amazonaws.com   443:30821/TCP,3023:30801/TCP,3026:32612/TCP,3024:31253/TCP   4m21s

# NAME                       READY   UP-TO-DATE   AVAILABLE   AGE
# deployment.apps/teleport   2/2     2            2           4m21s

# NAME                                  DESIRED   CURRENT   READY   AGE
# replicaset.apps/teleport-5cf46ddf5f   2         2         2       4m21s
```

<Admonition type="note">
  You'll need to change the existing DNS record for your `teleport` chart installation to point to your new `teleport-cluster`
  chart installation. You should point the DNS record to the external IP or hostname of the Kubernetes load balancer.

  (!docs/pages/kubernetes-access/helm/includes/kubernetes-externaladdress.mdx!)

  For testing, you can access the load balancer's IP or hostname directly. You may need to accept insecure warnings in your
  browser to view the page successfully.
</Admonition>

## Step 6/6. Remove the bootstrap data and update the chart deployment

Once you've tested your new Teleport cluster and you're confident that your data has been migrated successfully,
you should redeploy the chart without your backup data mounted for security.

<Tabs>
  <TabItem label="Using values.yaml">
  Edit your `teleport-cluster-values.yaml` file to remove `extraArgs`, `extraVolumes` and `extraVolumeMounts`:

  ```yaml
  chartMode: custom
  ```

  Upgrade the Helm deployment to use the new values:

  ```code
  $ helm upgrade teleport teleport/teleport-cluster \
    --namespace teleport-cluster \
    -f teleport-cluster-values.yaml
  ```

  </TabItem>
  <TabItem label="Using --set via CLI">
    ```code
    $ helm upgrade teleport teleport/teleport-cluster \
    --namespace teleport-cluster \
    --set chartMode=custom
    ```
  </TabItem>
</Tabs>

After this, delete the Kubernetes secret containing the backup data:

```code
$ kubectl --namespace delete secret/bootstrap
```

Finally, you should also delete the `backup.yaml` file from your local disk:

```code
$ rm -f backup.yaml
```

## Uninstalling Teleport

To uninstall the `teleport-cluster` chart, use `helm uninstall <release-name>`. For example:

```code
$ helm --namespace teleport-cluster uninstall teleport
```

## Next steps

To see all of the options you can set in the values file for the
`teleport-cluster` Helm chart, consult our [reference
guide](../../reference/helm-reference/teleport-cluster.mdx).

