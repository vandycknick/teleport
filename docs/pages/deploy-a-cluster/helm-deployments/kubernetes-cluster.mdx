---
title: Getting Started - Kubernetes with SSO
description: Getting started with Teleport. Let's deploy Teleport in a Kubernetes with SSO and Audit logs
---

<ScopedBlock title="Teleport Cloud customers" scope={["cloud"]}>
This guide shows you how to deploy the Teleport Auth Service and Proxy Service on a Kubernetes cluster. These services are fully managed in Teleport Cloud.

Instead, Teleport Cloud users should consult the following guide, which shows you how to connect a Teleport Kubernetes Service instance to an existing Teleport cluster:

</ScopedBlock>

Teleport can provide secure, unified access to your Kubernetes clusters. This guide will show you how to:

- Deploy Teleport in a Kubernetes cluster.
- Set up Single Sign-On (SSO) for authentication to your Teleport cluster.

While completing this guide, you will deploy a single Teleport pod running the Auth Service and Proxy Service in your Kubernetes cluster, and a load balancer that allows outside traffic to your Teleport cluster. Users can then access your Kubernetes cluster via the Teleport cluster running within it.

<Admonition type="tip" title="Have an existing Teleport cluster?">
If you are already running Teleport on another platform, you can use your existing Teleport deployment to access your Kubernetes cluster. [Follow our guide](../../kubernetes-access/getting-started.mdx) to connect your Kubernetes cluster to Teleport.
</Admonition>

(!docs/pages/includes/cloud/call-to-action.mdx!)

## Follow along with our video guide

<iframe
  width="712"
  height="400"
  src="https://www.youtube-nocookie.com/embed/VPGYLEMTdJ8?rel=0&modestbranding=1"
  frameBorder="0"
  allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
  allowFullScreen
/>

## Prerequisites
- A registered domain name. This is required for Teleport to set up TLS via Let's Encrypt and for Teleport clients to verify the Proxy Service host.
- A Kubernetes cluster hosted by a cloud provider, which is required for the load balancer we deploy in this guide.

<Admonition title="Local-only setups" type="tip">
Teleport also supports Kubernetes in on-premise and air-gapped environments. If you would like to try out Teleport on your local machine, we recommend following our [Docker Compose guide](../../try-out-teleport/docker-compose.mdx).
</Admonition>

(!docs/pages/includes/kubernetes-access/helm-k8s.mdx!)

(!docs/pages/includes/permission-warning.mdx!)

## Step 1/3. Install Teleport

Let's start with a single-pod Teleport deployment using a persistent volume as a backend. Modify the values of `CLUSTER_NAME` and `EMAIL` according to your environment, where `CLUSTER_NAME` is the domain name you are using for your Teleport deployment and `EMAIL` is an email address used for notifications.

(!docs/pages/kubernetes-access/helm/includes/helm-repo-add.mdx!)

<Tabs>
  <TabItem label="Open Source">
    ```code
    $ CLUSTER_NAME="tele.example.com"
    $ EMAIL="mail@example.com"

    # Install a single node teleport cluster and provision a cert using ACME.
    # Set clusterName to unique hostname, for example tele.example.com
    # Set acmeEmail to receive correspondence from Letsencrypt certificate authority.
    $ helm install teleport-cluster teleport/teleport-cluster \
      --create-namespace \
      --namespace=teleport-cluster \
      --set clusterName=${CLUSTER_NAME?} \
      --set acme=true \
      --set acmeEmail=${EMAIL?} \
      --version (=teleport.version=)
    ```
  </TabItem>

  <TabItem label="Enterprise" scope={["enterprise"]}>
    ```code
    $ CLUSTER_NAME="tele.example.com"
    $ EMAIL="mail@example.com"

    # Create a namespace for a deployment.
    $ kubectl create namespace teleport-cluster-ent

    # Set the kubectl context to the namespace to save some typing
    $ kubectl config set-context --current --namespace=teleport-cluster-ent

    # Get a license from Teleport and create a secret called "license" in the
    # namespace you created
    $ kubectl create secret generic license --from-file=license.pem

    # Install Teleport
    $ helm install teleport-cluster teleport/teleport-cluster \
      --namespace=teleport-cluster-ent \
      --version (=teleport.version=) \
      --set clusterName=${CLUSTER_NAME?} \
      --set acme=true \
      --set enterprise=true \
      --set acmeEmail=${EMAIL?}
    ```
  </TabItem>
</Tabs>

Teleport's Helm chart uses an [external load balancer](https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/)
to create a public IP for Teleport.

<Tabs>
  <TabItem label="Open Source">
    ```code
    # Set kubectl context to the namespace to save some typing
    $ kubectl config set-context --current --namespace=teleport-cluster

    # Service is up, load balancer is created
    $ kubectl get services
    # NAME               TYPE           CLUSTER-IP   EXTERNAL-IP      PORT(S)                        AGE
    # teleport-cluster   LoadBalancer   10.4.4.73    104.199.126.88   443:31204/TCP,3026:32690/TCP   89s

    # Save the pod IP or hostname.
    $ MYIP=$(kubectl get services teleport-cluster -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
    $ echo $MYIP
    # 192.168.2.1
    ```

    If `$MYIP` is blank, your cloud provider may have assigned a hostname to the load balancer rather than an IP address. Run the following command to retrieve the hostname, which you will use in place of `$MYIP` for subsequent commands.

    ```code
    $ MYIP=$(kubectl get services teleport-cluster -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
    ```
  </TabItem>

  <TabItem label="Enterprise" scope={["enterprise"]}>
    ```code
    # Set kubectl context to the namespace to set some typing
    $ kubectl config set-context --current --namespace=teleport-cluster-ent

    # Service is up, load balancer is created
    $ kubectl get services
    # NAME                   TYPE           CLUSTER-IP   EXTERNAL-IP      PORT(S)                        AGE
    # teleport-cluster-ent   LoadBalancer   10.4.4.73    104.199.126.88   443:31204/TCP,3026:32690/TCP   89s

    # Save the pod IP or hostname.
    $ MYIP=$(kubectl get services teleport-cluster-ent -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
    $ echo $MYIP
    # 192.168.2.1
    ```

    If `$MYIP` is blank, your cloud provider may have assigned a hostname to the load balancer rather than an IP address. Run the following command to retrieve the hostname, which you will use in place of `$MYIP` for subsequent commands.

    ```code
    $ MYIP=$(kubectl get services teleport-cluster -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
    ```
  </TabItem>
</Tabs>

(!docs/pages/includes/dns.mdx!)

Use the following command to confirm that Teleport is running:

```code
$ curl https://tele.example.com/webapi/ping

# {"server_version":"6.0.0","min_client_version":"3.0.0"}
```

## Step 2/3. Create a local user

Local users are a reliable fallback for cases when the SSO provider is down.
Let's create a local user `alice` who has access to Kubernetes group `system:masters`.

Save this role as `member.yaml`:

```yaml
kind: role
version: v5
metadata:
  name: member
spec:
  allow:
    kubernetes_groups: ["system:masters"]
    kubernetes_labels:
      '*': '*'
```

Create the role and add a user:

```code
# To create a local user, we are going to run Teleport's admin tool tctl from the pod.
$ POD=$(kubectl get pod -l app=teleport-cluster -o jsonpath='{.items[0].metadata.name}')

# Create a role
$ kubectl exec -i ${POD?} -- tctl create -f < member.yaml

# Generate an invite link for the user.
$ kubectl exec -ti ${POD?} -- tctl  users add alice --roles=member

# User "alice" has been created but requires a password. Share this URL with the user to
# complete user setup, link is valid for 1h:

# https://tele.example.com:443/web/invite/random-token-id-goes-here

# NOTE: Make sure tele.example.com:443 points at a Teleport proxy which users can access.
```

Let's install `tsh` and `tctl` on Linux.
For other install options, check out the [installation guide](../../installation.mdx)

<Tabs>
  <TabItem label="Open Source">
    ```code
    $ curl -L -O https://get.gravitational.com/teleport-v(=teleport.version=)-linux-amd64-bin.tar.gz
    $ tar -xzf teleport-v(=teleport.version=)-linux-amd64-bin.tar.gz
    $ sudo mv teleport/tsh /usr/local/bin/tsh
    $ sudo mv teleport/tctl /usr/local/bin/tctl
    ```
  </TabItem>

  <TabItem label="Enterprise" scope={["enterprise"]}>
    ```code
    $ curl -L -O https://get.gravitational.com/teleport-ent-v(=teleport.version=)-linux-amd64-bin.tar.gz
    $ tar -xzf teleport-ent-v(=teleport.version=)-linux-amd64-bin.tar.gz
    $ sudo mv teleport-ent/tsh /usr/local/bin/tsh
    $ sudo mv teleport-ent/tctl /usr/local/bin/tctl
    ```
  </TabItem>
</Tabs>

Try `tsh login` with a local user. Use a custom `KUBECONFIG` to prevent overwriting
the default one in case there is a problem.

```code
$ KUBECONFIG=${HOME?}/teleport.yaml tsh login --proxy=tele.example.com:443 --user=alice
```

Teleport updates `KUBECONFIG` with a short-lived 12-hour certificate.

```code
# List connected Kubernetes clusters
$ tsh kube ls

# Kube Cluster Name Selected
# ----------------- --------
# tele.example.com
# Login to Kubernetes by name
$ tsh kube login tele.example.com

# Once working, remove the KUBECONFIG= override to switch to teleport
$ KUBECONFIG=${HOME?}/teleport.yaml kubectl get -n teleport-cluster pods
# NAME                                READY   STATUS    RESTARTS   AGE
# teleport-cluster-6c9b88fd8f-glmhf   1/1     Running   0          127m
```

## Step 3/3. SSO for Kubernetes

In this step, we will set up the GitHub Single Sign-On connector for the OSS version of Teleport and Okta for the Enterprise version.

<Tabs>
  <TabItem label="Open Source">
    Save the file below as `github.yaml` and update the fields. You will need to set up the
    [GitHub OAuth 2.0 Connector](https://developer.github.com/apps/building-oauth-apps/creating-an-oauth-app/) app.
    Any member with the team `admin` in the organization `octocats` will be able to assume a builtin role `access`.

    ```yaml
    kind: github
    version: v3
    metadata:
      # connector name that will be used with `tsh --auth=github login`
      name: github
    spec:
      # client ID of your GitHub OAuth app
      client_id: client-id
      # client secret of your GitHub OAuth app
      client_secret: client-secret
      # This name will be shown on UI login screen
      display: GitHub
      # Change tele.example.com to your domain name
      redirect_url: https://tele.example.com:443/v1/webapi/github/callback
      # Map github teams to teleport roles
      teams_to_roles:
        - organization: octocats # GitHub organization name
          team: admin            # GitHub team name within that organization
          # map GitHub's "admin" team to Teleport's "access" role
          roles: ["access"]
    ```
  </TabItem>

  <TabItem label="Enterprise" scope={["enterprise"]}>
    Follow the [SAML Okta Guide](../../access-controls/sso/okta.mdx#configure-okta) to create a SAML app.
    Check out [OIDC guides](../../access-controls/sso/oidc.mdx#identity-providers) for OpenID Connect apps.
    Save the file below as `okta.yaml` and update the `acs` field.
    Any member in Okta group `okta-admin` will assume a builtin role `access`.

    ```yaml
    kind: saml
    version: v2
    metadata:
      name: okta
    spec:
      acs: https://tele.example.com/v1/webapi/saml/acs
      attributes_to_roles:
      - {name: "groups", value: "okta-admin", roles: ["access"]}
      entity_descriptor: |
        <?xml !!! Make sure to shift all lines in XML descriptor
        with 4 spaces, otherwise things will not work
    ```
  </TabItem>
</Tabs>

To create a connector, we are going to run Teleport's admin tool `tctl` from the pod.

<Tabs>
  <TabItem label="Open Source">
    ```code
    $ kubectl config set-context --current --namespace=teleport-cluster
    $ POD=$(kubectl get po -l app=teleport-cluster -o jsonpath='{.items[0].metadata.name}')

    $ kubectl exec -i ${POD?} -- tctl create -f < github.yaml
    # authentication connector "github" has been created
    ```
  </TabItem>

  <TabItem label="Enterprise"  scope={["enterprise"]}>
    ```code
    # To create an Okta connector, we are going to run Teleport's admin tool tctl from the pod.
    $ POD=$(kubectl get po -l app=teleport-cluster-ent -o jsonpath='{.items[0].metadata.name}')

    $ kubectl exec -i ${POD?} -- tctl create -f < okta.yaml
    # authentication connector 'okta' has been created
    ```
  </TabItem>
</Tabs>

Try `tsh login` with a GitHub user. This example uses a custom `KUBECONFIG` to prevent overwriting
the default one in case there is a problem.

<Tabs>
  <TabItem label="Open Source">
    ```code
    $ KUBECONFIG=${HOME?}/teleport.yaml tsh login --proxy=tele.example.com --auth=github
    ```
  </TabItem>

  <TabItem label="Enterprise" scope={["enterprise"]}>
    ```code
    $ KUBECONFIG=${HOME?}/teleport.yaml tsh login --proxy=tele.example.com --auth=okta
    ```
  </TabItem>
</Tabs>

<Admonition
  type="note"
  title="Debugging SSO"
>
  If you are getting a login error, take a look at the audit log for details:

  ```code
  $ kubectl exec -ti "${POD?}" -- tail -n 100 /var/lib/teleport/log/events.log

  # {"error":"user \"alice\" does not belong to any teams configured in \"github\" connector","method":"github","attributes":{"octocats":["devs"]}}
  ```
</Admonition>

## Next steps

To see all of the options you can set in the values file for the
`teleport-cluster` Helm chart, consult our [reference
guide](../../reference/helm-reference/teleport-cluster.mdx).

Read our guides to additional ways you can protect Kubernetes clusters with
Teleport:

- [Connect Multiple Kubernetes Clusters](../../kubernetes-access/guides/multiple-clusters.mdx)
- [Set up Machine ID with Kubernetes](../../machine-id/guides/kubernetes.mdx)
- [Federated Access using Trusted Clusters](../../kubernetes-access/guides/federation.mdx)
- [Single-Sign On and Kubernetes Access Control](../../kubernetes-access/controls.mdx)
